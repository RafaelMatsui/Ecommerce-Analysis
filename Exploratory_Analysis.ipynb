{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Olist Data Preparation\n*By Rafael LeRoy*","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n\nOur goal for this task is to clean and prepare the data for usage in a dashboard.\n\nOur Dashboard will consist of three views:\n\n**Orders:** \n* Which customers ordered the most\n* Which geographical states did most orders come from\n* How many orders were there per month\n\n**Sellers:**\n* Which sellers had the highest sales revenue\n* Which geographical states did most sellers come from\n* How many sellers were there per month\n\n**Products:**\n* Which product categories sold the most\n* Which indivisual products sold the most\n\nWe will prepare the data in a way that will most suitably accommodate these views.","metadata":{}},{"cell_type":"markdown","source":"## Getting an overview of the Data\n\nBefore we begin any of the preparation, it is important that we get an overview of the datasets","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom bs4 import BeautifulSoup\nimport requests\nimport matplotlib.pyplot as plt\nfrom googletrans import Translator\n\npd.set_option('display.max_columns', 500)\nplt.rcParams['axes.grid'] = True\n\ncustomers = pd.read_csv('brazilian-ecommerce/olist_customers_dataset.csv')\nsellers = pd.read_csv('brazilian-ecommerce/olist_sellers_dataset.csv')\nproducts = pd.read_csv('brazilian-ecommerce/olist_products_dataset.csv')\norder_items = pd.read_csv('brazilian-ecommerce/olist_order_items_dataset.csv')\norders = pd.read_csv('brazilian-ecommerce/olist_orders_dataset.csv')\norder_payments = pd.read_csv('brazilian-ecommerce/olist_order_payments_dataset.csv')\nreviews = pd.read_csv('brazilian-ecommerce/olist_order_reviews_dataset.csv')","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-06-05T18:23:24.392164Z","iopub.execute_input":"2022-06-05T18:23:24.392614Z","iopub.status.idle":"2022-06-05T18:23:25.975564Z","shell.execute_reply.started":"2022-06-05T18:23:24.392580Z","shell.execute_reply":"2022-06-05T18:23:25.974358Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Lets summarize the null values and duplicate values in our datasets\n\ndef series_describer(df):\n    null_dic = df.isna().sum().to_dict()\n    null_columns = ', '.join([i for i in null_dic if null_dic[i] != 0])\n    null_percentage = len(df[df.isnull().any(axis = 1)]) / len(df) * 100\n    return [len(df.columns), len(df), df.duplicated().sum(), len(df[df.isnull().any(axis = 1)]), null_percentage, null_columns]\n\ndf = pd.DataFrame([series_describer(i) for i in [customers, sellers, products, order_items, orders, order_payments, reviews]],\n                  columns = ['Number Of columns', 'Number of Rows', 'Duplicated Rows', 'Number of Nulls','Null_percentage','Null Columns'],\n                  index = ['customers', 'sellers', 'products', 'order_items','orders', 'order_payments', 'reviews'])\n\ndf.style.background_gradient()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:23:25.976725Z","iopub.execute_input":"2022-06-05T18:23:25.977096Z","iopub.status.idle":"2022-06-05T18:23:26.865013Z","shell.execute_reply.started":"2022-06-05T18:23:25.977035Z","shell.execute_reply":"2022-06-05T18:23:26.864103Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"The table above shows us the number of rows with at least one null value. Other than the reviews dataset which we will deal with later, this data seems to have very few null values meaning the data can be used to create an accurate analysis.","metadata":{}},{"cell_type":"markdown","source":"## Preparing the products dataset","metadata":{}},{"cell_type":"code","source":"# First lets view the Null values in this dataset\nproducts.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:23:26.866759Z","iopub.execute_input":"2022-06-05T18:23:26.867102Z","iopub.status.idle":"2022-06-05T18:23:26.877750Z","shell.execute_reply.started":"2022-06-05T18:23:26.867049Z","shell.execute_reply":"2022-06-05T18:23:26.876869Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"From the cell above we can see the `product_name_lenght` and `product_description_lenght` are mispelled so we will correct them.","metadata":{}},{"cell_type":"code","source":"products.rename(columns = {'product_name_lenght': 'product_name_length',\n                           'product_description_lenght': 'product_description_length'}, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:23:26.878997Z","iopub.execute_input":"2022-06-05T18:23:26.879555Z","iopub.status.idle":"2022-06-05T18:23:26.886863Z","shell.execute_reply.started":"2022-06-05T18:23:26.879522Z","shell.execute_reply":"2022-06-05T18:23:26.886212Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"We can also see the number of null values for every column in our dataset. The last four columns all contain quantative data so the null values can be imputed with either the median or the mean.\n\nLets explore the best option for imputation.","metadata":{}},{"cell_type":"code","source":"products_to_plot = products[['product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']]\nproducts_to_plot.plot(subplots=True, layout=(4,4), kind='box', figsize=(20,20), patch_artist = True)\nplt.grid()\nplt.subplots_adjust(wspace=0.5)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:23:26.887799Z","iopub.execute_input":"2022-06-05T18:23:26.888733Z","iopub.status.idle":"2022-06-05T18:23:27.727564Z","shell.execute_reply.started":"2022-06-05T18:23:26.888699Z","shell.execute_reply":"2022-06-05T18:23:27.726695Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"From the boxplots above, we can see the all four of the columns have many outliers as a rightward skew. As we dont want these outliers represented within our average the best option here would be to impute the median.","metadata":{}},{"cell_type":"code","source":"cols = ['product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']\n\nproducts[cols] = products[cols].fillna(products[cols].median())\n\nproducts.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:23:27.728773Z","iopub.execute_input":"2022-06-05T18:23:27.729615Z","iopub.status.idle":"2022-06-05T18:23:27.750511Z","shell.execute_reply.started":"2022-06-05T18:23:27.729581Z","shell.execute_reply":"2022-06-05T18:23:27.749566Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"We now have to move on to the columns with the nominal data. This is where things get a bit tricky. As the data is qualitative, we can't impute with the mean or median and we would have to use the mode. One of of the key objects of the task assigned to us is to analyse the performance of the different products. This means that for this analysis the accuracy of the product_category is incredibly important. For this reason I will just drop these rows instead of imputing the mode as I want the data to be as accurate as possible.","metadata":{}},{"cell_type":"code","source":"products.dropna(inplace = True)\nproducts.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:23:27.751659Z","iopub.execute_input":"2022-06-05T18:23:27.752050Z","iopub.status.idle":"2022-06-05T18:23:27.770944Z","shell.execute_reply.started":"2022-06-05T18:23:27.752021Z","shell.execute_reply":"2022-06-05T18:23:27.770286Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"**Now that the null values have been dealt with, we can move onto feature engineering.**","metadata":{}},{"cell_type":"code","source":"#Lets first take a look at the unique values for every column.\n{k: products[k].unique() for k in products.columns}","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:23:27.772083Z","iopub.execute_input":"2022-06-05T18:23:27.772990Z","iopub.status.idle":"2022-06-05T18:23:27.796029Z","shell.execute_reply.started":"2022-06-05T18:23:27.772955Z","shell.execute_reply":"2022-06-05T18:23:27.795216Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"**The unique values above show us which columns we could engineer:**\n * The values in the `product_category_name` column are in portuguese. We should translate to english","metadata":{}},{"cell_type":"code","source":"# Cleaning up the product category names to remove the underscores\nproducts['product_category_name'] = products['product_category_name'].str.replace('_', ' ')\n\n#Translating from Portugease to English\n#Lets translate the product_category_column from Portugease to English\ntranslator = Translator()\ntranslated_dic = {i: translator.translate(i, dest = 'en').text for i in products['product_category_name'].unique()}\ntranslated_dic","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:23:27.798248Z","iopub.execute_input":"2022-06-05T18:23:27.798554Z","iopub.status.idle":"2022-06-05T18:24:28.904497Z","shell.execute_reply.started":"2022-06-05T18:23:27.798529Z","shell.execute_reply":"2022-06-05T18:24:28.903595Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Some of the values have incorrectly translated so we will change those manually\n\ntranslated_dic['informatica acessorios'] = 'Computer Accessories'\ntranslated_dic['malas acessorios'] = 'Accessory Bags'\ntranslated_dic['fashion calcados'] = 'Fashion Shoes'\ntranslated_dic['telefonia'] = 'Telephone'\ntranslated_dic['eletroportateis'] = 'Small Appliances'\ntranslated_dic['Climatizacao'] = 'Air Conditioning'\ntranslated_dic['telefonia fixa'] = 'Telephony Fix'\ntranslated_dic['eletrodomesticos 2'] = 'Home Appliances 2'\ntranslated_dic['portateis casa forno e cafe'] = 'Portable Home Oven and Coffee'\ntranslated_dic['portateis cozinha e preparadores de alimentos'] = 'Portable Cooking and Food Preparers'\ntranslated_dic['flores'] = 'Flowers'\ntranslated_dic['moveis colchao e estofado'] = 'mattress and upholstery furniture'\n\ntranslated_dic =  {k:v.title() for k,v in translated_dic.items()}\n\nproducts['product_category_name'] = products['product_category_name'].map(translated_dic)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:28.905722Z","iopub.execute_input":"2022-06-05T18:24:28.906149Z","iopub.status.idle":"2022-06-05T18:24:28.919932Z","shell.execute_reply.started":"2022-06-05T18:24:28.906109Z","shell.execute_reply":"2022-06-05T18:24:28.919113Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"**Finally, we will check for any irregular values that may need to be removed**\n\nIf the data has any extreme outliars or any impossible values then they will have to be removed","metadata":{}},{"cell_type":"code","source":"quantative_rows = products.iloc[:, 2:]\nquantative_rows.plot(subplots=True, layout=(4,8), kind='box', figsize=(20,18), patch_artist=True)\nplt.subplots_adjust(wspace=0.5)\nprint(quantative_rows.min(axis=0))","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:28.921142Z","iopub.execute_input":"2022-06-05T18:24:28.921429Z","iopub.status.idle":"2022-06-05T18:24:30.208176Z","shell.execute_reply.started":"2022-06-05T18:24:28.921403Z","shell.execute_reply":"2022-06-05T18:24:30.207527Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"From the Boxplots above we can see that while there are a lot of outliars, given the context of the columns, none are inexplainable. \n\nWhen looking at the minimum value for each column however, we can see that `product_weight_g` has a minimum of 0 which should be impossible. Lets explore the `product_weight_g` column to try and solve the problem.","metadata":{}},{"cell_type":"code","source":"products[products['product_weight_g'] == 0]","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:30.209163Z","iopub.execute_input":"2022-06-05T18:24:30.209535Z","iopub.status.idle":"2022-06-05T18:24:30.228534Z","shell.execute_reply.started":"2022-06-05T18:24:30.209508Z","shell.execute_reply":"2022-06-05T18:24:30.227642Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"The cell above shows every row in which the `product_weight_g` is 0. It is impossible for a physical object to have no weight therefore we will remove these rows.","metadata":{}},{"cell_type":"markdown","source":"## Preparing the sellers dataset","metadata":{}},{"cell_type":"code","source":"# First lets view the Null values in this dataset\nsellers.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:30.229909Z","iopub.execute_input":"2022-06-05T18:24:30.230490Z","iopub.status.idle":"2022-06-05T18:24:30.238645Z","shell.execute_reply.started":"2022-06-05T18:24:30.230451Z","shell.execute_reply":"2022-06-05T18:24:30.238095Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"As there are no null values, we can move straight onto feature engineering.","metadata":{}},{"cell_type":"code","source":"#Lets first take a look at the unique values for every column.\n{k: sellers[k].unique() for k in sellers.columns}","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:30.239676Z","iopub.execute_input":"2022-06-05T18:24:30.240329Z","iopub.status.idle":"2022-06-05T18:24:30.252634Z","shell.execute_reply.started":"2022-06-05T18:24:30.240297Z","shell.execute_reply":"2022-06-05T18:24:30.251649Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"**The unique values above show us which columns we could engineer:**\n * The values in the `seller_state` column have the abbriviations for the state names. To create map charts in Tableau, we would have to replace these abbriviations with the states full name","metadata":{}},{"cell_type":"code","source":"# Scraping the State names from wikipedia\nbrazilian_cities_url = 'https://en.wikipedia.org/wiki/Federative_units_of_Brazil'\nres = requests.get(brazilian_cities_url)\nsoup = BeautifulSoup(res.text, \"html.parser\")\ntable = soup.find_all('table')\nstate_names = pd.read_html(str(table[1]))[0]\n\ndic = pd.Series(state_names['Flag and name'].values,index=state_names['Code']).to_dict()\nsellers['seller_state'] = sellers['seller_state'].map(dic)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:30.254298Z","iopub.execute_input":"2022-06-05T18:24:30.254911Z","iopub.status.idle":"2022-06-05T18:24:31.607515Z","shell.execute_reply.started":"2022-06-05T18:24:30.254871Z","shell.execute_reply":"2022-06-05T18:24:31.606729Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"sellers.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:31.608535Z","iopub.execute_input":"2022-06-05T18:24:31.608820Z","iopub.status.idle":"2022-06-05T18:24:31.618260Z","shell.execute_reply.started":"2022-06-05T18:24:31.608795Z","shell.execute_reply":"2022-06-05T18:24:31.617612Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Preparing the customers dataset","metadata":{}},{"cell_type":"code","source":"# First lets view the Null values in this dataset\ncustomers.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:31.619559Z","iopub.execute_input":"2022-06-05T18:24:31.619927Z","iopub.status.idle":"2022-06-05T18:24:31.649212Z","shell.execute_reply.started":"2022-06-05T18:24:31.619898Z","shell.execute_reply":"2022-06-05T18:24:31.648200Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"As there are no null values, we can move straight onto feature engineering.","metadata":{}},{"cell_type":"code","source":"# Lets take a look at the unique values for every column.\n{k: customers[k].unique() for k in customers.columns}","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:31.650530Z","iopub.execute_input":"2022-06-05T18:24:31.650823Z","iopub.status.idle":"2022-06-05T18:24:31.714072Z","shell.execute_reply.started":"2022-06-05T18:24:31.650798Z","shell.execute_reply":"2022-06-05T18:24:31.713085Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"**The unique values above show us which columns we could engineer:**\n * much like the `seller_state` column the `customer_state` column also has the abbriviations for the state names. We will have to change these\n * Using the `customer_unique_id column`, we could create a boolean column which would be used to find returning customers.","metadata":{}},{"cell_type":"code","source":"# Using the previously defined dictionary, we can change the customer_state values\ncustomers['customer_state'] = customers['customer_state'].map(dic)\ncustomers['customer_state'].head()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:31.715552Z","iopub.execute_input":"2022-06-05T18:24:31.715970Z","iopub.status.idle":"2022-06-05T18:24:31.732727Z","shell.execute_reply.started":"2022-06-05T18:24:31.715921Z","shell.execute_reply":"2022-06-05T18:24:31.731673Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Creating a column to find returning customers\ncustomers['repeat_customer'] = customers['customer_unique_id'].duplicated(keep = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:31.734119Z","iopub.execute_input":"2022-06-05T18:24:31.734456Z","iopub.status.idle":"2022-06-05T18:24:31.759156Z","shell.execute_reply.started":"2022-06-05T18:24:31.734427Z","shell.execute_reply":"2022-06-05T18:24:31.758293Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Preparing the order_items dataset","metadata":{}},{"cell_type":"code","source":"# # First lets view the Null values in this dataset\norder_items.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:31.760329Z","iopub.execute_input":"2022-06-05T18:24:31.760648Z","iopub.status.idle":"2022-06-05T18:24:31.798080Z","shell.execute_reply.started":"2022-06-05T18:24:31.760612Z","shell.execute_reply":"2022-06-05T18:24:31.797181Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"As there are no null values, we can move straight onto feature engineering.","metadata":{}},{"cell_type":"code","source":"{k: order_items[k].unique() for k in order_items.columns}","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:31.799562Z","iopub.execute_input":"2022-06-05T18:24:31.799955Z","iopub.status.idle":"2022-06-05T18:24:31.874693Z","shell.execute_reply.started":"2022-06-05T18:24:31.799917Z","shell.execute_reply":"2022-06-05T18:24:31.873794Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"**The unique values above show us which columns we could engineer:**\n * We will first check if the `shipping_limit_date` is in a format that Tableau supports. yyyy-MM-dd HH:mm:ss is supported by Tableau therefore there is no issue with that column.","metadata":{}},{"cell_type":"markdown","source":"**Finally, we will check for any irregular values that may need to be removed**\n\nIf the data has any extreme outliars or any impossible values then they will have to be removed","metadata":{"execution":{"iopub.status.busy":"2022-05-30T23:22:27.415042Z","iopub.execute_input":"2022-05-30T23:22:27.415413Z","iopub.status.idle":"2022-05-30T23:22:27.421303Z","shell.execute_reply.started":"2022-05-30T23:22:27.415384Z","shell.execute_reply":"2022-05-30T23:22:27.420075Z"}}},{"cell_type":"code","source":"years = order_items['shipping_limit_date'].apply(lambda x: x.split('-')[0])\nquantative_rows = order_items.loc[:,['price', 'freight_value']]\nquantative_rows['Year'] = years\n\nfig, ax = plt.subplots(1, 2, figsize = (16, 4))\nsns.histplot(order_items['price'], ax = ax[0], bins = 50)\nsns.histplot(order_items['freight_value'], ax = ax[1], bins = 50)\nax[0].axvline(order_items['price'].median(), color = 'red')\nax[1].axvline(order_items['freight_value'].median(), color = 'red')\nfig.show()\npd.DataFrame({'min_value': quantative_rows.min(axis = 0), 'max_value' :quantative_rows.max(axis = 0)})","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:31.876029Z","iopub.execute_input":"2022-06-05T18:24:31.876842Z","iopub.status.idle":"2022-06-05T18:24:32.616631Z","shell.execute_reply.started":"2022-06-05T18:24:31.876796Z","shell.execute_reply":"2022-06-05T18:24:32.615802Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"Above we have plotted histograms for `price` and `freight_value` columns. While the data for both columns do have a rightward skew with many outliars, the data tends to show what we would expect with most orders being less that 500 dollars and most shipping costs being below 50 dollars.\n\nWe have also displayed the minimum and maximum values of both columns and there does not seem to be any inexplicable values.","metadata":{}},{"cell_type":"markdown","source":"## Preparing the orders dataset","metadata":{}},{"cell_type":"code","source":"# First lets view the Null values in this dataset\norders.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:32.618964Z","iopub.execute_input":"2022-06-05T18:24:32.619888Z","iopub.status.idle":"2022-06-05T18:24:32.660839Z","shell.execute_reply.started":"2022-06-05T18:24:32.619850Z","shell.execute_reply":"2022-06-05T18:24:32.660212Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"This dataset seems to have a large number of null values. Lets try and work out why this is.","metadata":{}},{"cell_type":"code","source":"# Viewing the null values in the order_delivered_customer_date column\n\norders[orders['order_delivered_customer_date'].isna()]['order_status'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:32.661858Z","iopub.execute_input":"2022-06-05T18:24:32.662206Z","iopub.status.idle":"2022-06-05T18:24:32.680708Z","shell.execute_reply.started":"2022-06-05T18:24:32.662177Z","shell.execute_reply":"2022-06-05T18:24:32.679769Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"From furthur inspection, we can the null values in this dataset fall almost entirely on orders which have not yet been delivered. As you can't provide information on an event that has not yet happened, some of the values have been left blank. As we now know that these nulls aren't due to any error, we can leave them. Having said this however, there is no explination for the nulls values where the `order_status` is delivered so we will remove those rows.","metadata":{}},{"cell_type":"code","source":"drop_index = orders[(orders['order_status'] == 'delivered') & (orders.isnull().any(axis = 1))].index\norders = orders.drop(drop_index).reindex()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:32.682009Z","iopub.execute_input":"2022-06-05T18:24:32.682712Z","iopub.status.idle":"2022-06-05T18:24:32.768271Z","shell.execute_reply.started":"2022-06-05T18:24:32.682668Z","shell.execute_reply":"2022-06-05T18:24:32.767532Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## Preparing the order_payments dataset","metadata":{}},{"cell_type":"code","source":"# First lets view the Null values in this dataset\norder_payments.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:32.770990Z","iopub.execute_input":"2022-06-05T18:24:32.771425Z","iopub.status.idle":"2022-06-05T18:24:32.788111Z","shell.execute_reply.started":"2022-06-05T18:24:32.771397Z","shell.execute_reply":"2022-06-05T18:24:32.787434Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"The are no null values so lets move onto checking fr any outliars","metadata":{}},{"cell_type":"code","source":"quantative_rows = order_payments[['payment_sequential', 'payment_installments', 'payment_value']]\nquantative_rows.plot(subplots=True, layout=(4,4), kind='box', figsize=(20,20), patch_artist = True)\nplt.grid()\nplt.subplots_adjust(wspace=0.5)\nplt.show();\n\npd.DataFrame({'min_value': quantative_rows.min(axis = 0), 'max_value' :quantative_rows.max(axis = 0),\n              'median_value' :quantative_rows.median(axis = 0), 'mean_value': quantative_rows.mean(axis = 0)})","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:32.789255Z","iopub.execute_input":"2022-06-05T18:24:32.789685Z","iopub.status.idle":"2022-06-05T18:24:33.610196Z","shell.execute_reply.started":"2022-06-05T18:24:32.789657Z","shell.execute_reply":"2022-06-05T18:24:33.609280Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"While our boxplots show many outliars for every column, given the context there shouldn't be any cause for concern.\n\nWhat should be looked at however is the `payment_value` and `payment_installments` columns having  minimum values of 0.","metadata":{}},{"cell_type":"code","source":"# Isolate rows for which either payment_sequential or payment_value is 0\norder_payments[(order_payments['payment_value'] == 0) | (order_payments['payment_installments'] == 0)]","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:33.612935Z","iopub.execute_input":"2022-06-05T18:24:33.613702Z","iopub.status.idle":"2022-06-05T18:24:33.628303Z","shell.execute_reply.started":"2022-06-05T18:24:33.613655Z","shell.execute_reply":"2022-06-05T18:24:33.627348Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"From the cell above, we can see that for the rows in which the `payment_value` is 0, the `payment_type` is either a voucher or some unspecified payment_type meaning that the customer not spending any money is viable. For that reason, we will leave these hows.\n\nThere is no explination however for the `payment_installments` being 0 so for that reason we will drop those rows.","metadata":{}},{"cell_type":"code","source":"order_payments = order_payments[order_payments['payment_installments'] != 0]","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:33.629801Z","iopub.execute_input":"2022-06-05T18:24:33.630255Z","iopub.status.idle":"2022-06-05T18:24:33.641924Z","shell.execute_reply.started":"2022-06-05T18:24:33.630214Z","shell.execute_reply":"2022-06-05T18:24:33.641128Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"## Preparing the reviews_dataset","metadata":{}},{"cell_type":"code","source":"# First lets view the Null values in this dataset\nreviews.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:33.643664Z","iopub.execute_input":"2022-06-05T18:24:33.644479Z","iopub.status.idle":"2022-06-05T18:24:33.677852Z","shell.execute_reply.started":"2022-06-05T18:24:33.644444Z","shell.execute_reply":"2022-06-05T18:24:33.676941Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"The null values in this dataframe are only in the `review_comment_title` column and the `review_comment_message` section. It is plausible that leaving a message to go with your review is optional meaning that a lot of people would have just left this section blank. Because of this, we will leave these null values.","metadata":{}},{"cell_type":"code","source":"answer_year = reviews['review_answer_timestamp'].apply(lambda x: float(x.split('-')[0]))\ncreation_year = reviews['review_creation_date'].apply(lambda x: float(x.split('-')[0]))\n\nquantative_rows = reviews.loc[:, ['review_score']]\nquantative_rows['answer_year'] = answer_year\nquantative_rows['creation_year'] = creation_year\n\npd.DataFrame({'min' : quantative_rows.min(axis = 0), 'max': quantative_rows.max(axis = 0),\n             'mean': quantative_rows.mean(axis = 0), 'Median': quantative_rows.median(axis = 0)})","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:33.679043Z","iopub.execute_input":"2022-06-05T18:24:33.679399Z","iopub.status.idle":"2022-06-05T18:24:33.814658Z","shell.execute_reply.started":"2022-06-05T18:24:33.679371Z","shell.execute_reply":"2022-06-05T18:24:33.813732Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"## Merging the Datasets","metadata":{}},{"cell_type":"code","source":"olist = customers.merge(orders, on = 'customer_id', how = 'left')\nolist = olist.merge(order_payments, on = 'order_id', how = 'left')\nolist = olist.merge(order_items, on = 'order_id', how = 'left')\nolist = olist.merge(products, on = 'product_id', how = 'left')\nolist = olist.merge(sellers, on = 'seller_id', how = 'left')\nolist = olist.merge(reviews, on = 'order_id', how = 'left')","metadata":{"execution":{"iopub.status.busy":"2022-06-05T18:24:33.815796Z","iopub.execute_input":"2022-06-05T18:24:33.816107Z","iopub.status.idle":"2022-06-05T18:24:34.656715Z","shell.execute_reply.started":"2022-06-05T18:24:33.816078Z","shell.execute_reply":"2022-06-05T18:24:34.655938Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"## We have now successfully prepared the data","metadata":{}}]}